{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install traker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T20:30:14.126347Z",
     "iopub.status.busy": "2025-01-04T20:30:14.126027Z",
     "iopub.status.idle": "2025-01-04T20:30:17.091585Z",
     "shell.execute_reply": "2025-01-04T20:30:17.090704Z",
     "shell.execute_reply.started": "2025-01-04T20:30:14.126319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trak import TRAKer\n",
    "\n",
    "def get_trak_matrix(\n",
    "    train_dl, val_dl, model, ckpts, train_set_size, val_set_size, **kwargs\n",
    "):\n",
    "    if kwargs is None or kwargs.get(\"task\") is None:\n",
    "        task = \"image_classification\"\n",
    "    else:\n",
    "        task = kwargs.pop(\"task\")\n",
    "\n",
    "    traker = TRAKer(model=model, task=task, train_set_size=train_set_size, **kwargs)\n",
    "\n",
    "    for model_id, checkpoint in enumerate(ckpts):\n",
    "        traker.load_checkpoint(checkpoint, model_id=model_id)\n",
    "        for batch in train_dl:\n",
    "            batch = [x.cuda() for x in batch]\n",
    "            # batch should be a tuple/list of inputs and labels\n",
    "            traker.featurize(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "    traker.finalize_features()\n",
    "\n",
    "    for model_id, checkpoint in enumerate(ckpts):\n",
    "        traker.start_scoring_checkpoint(\n",
    "            exp_name=\"test\",\n",
    "            checkpoint=checkpoint,\n",
    "            model_id=model_id,\n",
    "            num_targets=val_set_size,\n",
    "        )\n",
    "    for batch in val_dl:\n",
    "        batch = [x.cuda() for x in batch]\n",
    "        traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "    scores = traker.finalize_scores(exp_name=\"test\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T20:30:21.048127Z",
     "iopub.status.busy": "2025-01-04T20:30:21.047697Z",
     "iopub.status.idle": "2025-01-04T20:30:21.061004Z",
     "shell.execute_reply": "2025-01-04T20:30:21.060211Z",
     "shell.execute_reply.started": "2025-01-04T20:30:21.048100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class DDA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        checkpoints,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        group_indices,\n",
    "        train_set_size=None,\n",
    "        val_set_size=None,\n",
    "        trak_scores=None,\n",
    "        trak_kwargs=None,\n",
    "        device=\"cuda\",\n",
    "    ) -> None:\n",
    "        \n",
    "        self.model = model\n",
    "        self.checkpoints = checkpoints\n",
    "        self.dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "        self.group_indices = group_indices\n",
    "        self.device = device\n",
    "\n",
    "        if trak_scores is not None:\n",
    "            self.trak_scores = trak_scores\n",
    "        else:\n",
    "            try:\n",
    "                self.train_set_size = len(train_dataloader.dataset)\n",
    "                self.val_set_size = len(val_dataloader.dataset)\n",
    "            except AttributeError as e:\n",
    "                print(\n",
    "                    f\"No dataset attribute found in train_dataloader or val_dataloader. {e}\"\n",
    "                )\n",
    "                if train_set_size is None or val_set_size is None:\n",
    "                    raise ValueError(\n",
    "                        \"train_set_size and val_set_size must be specified if \"\n",
    "                        \"train_dataloader and val_dataloader do not have a \"\n",
    "                        \"dataset attribute.\"\n",
    "                    ) from e\n",
    "                self.train_set_size = train_set_size\n",
    "                self.val_set_size = val_set_size\n",
    "\n",
    "            # Step 1: compute TRAK scores\n",
    "            if trak_kwargs is not None:\n",
    "                trak_scores = get_trak_matrix(\n",
    "                    train_dl=self.dataloaders[\"train\"],\n",
    "                    val_dl=self.dataloaders[\"val\"],\n",
    "                    model=self.model,\n",
    "                    ckpts=self.checkpoints,\n",
    "                    train_set_size=self.train_set_size,\n",
    "                    val_set_size=self.val_set_size,\n",
    "                    **trak_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                trak_scores = get_trak_matrix(\n",
    "                    train_dl=self.dataloaders[\"train\"],\n",
    "                    val_dl=self.dataloaders[\"val\"],\n",
    "                    model=self.model,\n",
    "                    ckpts=self.checkpoints,\n",
    "                    train_set_size=self.train_set_size,\n",
    "                    val_set_size=self.val_set_size,\n",
    "                )\n",
    "\n",
    "            self.trak_scores = trak_scores\n",
    "\n",
    "    def get_group_losses(self, model, val_dl, group_indices) -> list:\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dl:\n",
    "                outputs = model(inputs.to(self.device))\n",
    "                loss = F.cross_entropy(\n",
    "                    outputs, labels.to(self.device), reduction=\"none\"\n",
    "                )\n",
    "                losses.append(loss)\n",
    "        losses = torch.cat(losses)\n",
    "\n",
    "        n_groups = len(set(group_indices))\n",
    "        group_losses = [losses[group_indices == i].mean() for i in range(n_groups)]\n",
    "        return group_losses\n",
    "\n",
    "    def compute_group_alignment_scores(self, trak_scores, group_indices, group_losses):\n",
    "        n_groups = len(set(group_indices))\n",
    "        S = np.array(trak_scores)\n",
    "        g = [\n",
    "            group_losses[i].cpu().numpy() * S[:, np.array(group_indices) == i].mean(axis=1)\n",
    "            for i in range(n_groups)\n",
    "        ]\n",
    "        g = np.stack(g)\n",
    "        group_alignment_scores = g.mean(axis=0)\n",
    "        return group_alignment_scores\n",
    "\n",
    "    def get_debiased_train_indices(\n",
    "        self, group_alignment_scores, use_heuristic=True, num_to_discard=None\n",
    "    ):\n",
    "        if use_heuristic:\n",
    "            return [i for i, score in enumerate(group_alignment_scores) if score >= 0]\n",
    "\n",
    "        if num_to_discard is None:\n",
    "            raise ValueError(\"num_to_discard must be specified if not using heuristic.\")\n",
    "\n",
    "        sorted_indices = sorted(\n",
    "            range(len(group_alignment_scores)),\n",
    "            key=lambda i: group_alignment_scores[i],\n",
    "        )\n",
    "        return sorted_indices[num_to_discard:]\n",
    "\n",
    "    def debias(self, use_heuristic=True, num_to_discard=None):\n",
    "        group_losses = self.get_group_losses(\n",
    "            model=self.model,\n",
    "            val_dl=self.dataloaders[\"val\"],\n",
    "            group_indices=self.group_indices,\n",
    "        )\n",
    "\n",
    "        group_alignment_scores = self.compute_group_alignment_scores(\n",
    "            self.trak_scores, self.group_indices, group_losses\n",
    "        )\n",
    "        \n",
    "        debiased_train_inds = self.get_debiased_train_indices(\n",
    "            group_alignment_scores,\n",
    "            use_heuristic=use_heuristic,\n",
    "            num_to_discard=num_to_discard,\n",
    "        )\n",
    "\n",
    "        return debiased_train_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T20:30:25.542237Z",
     "iopub.status.busy": "2025-01-04T20:30:25.541936Z",
     "iopub.status.idle": "2025-01-04T20:30:25.545684Z",
     "shell.execute_reply": "2025-01-04T20:30:25.544852Z",
     "shell.execute_reply.started": "2025-01-04T20:30:25.542213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T22:36:23.420256Z",
     "iopub.status.busy": "2025-01-03T22:36:23.419972Z",
     "iopub.status.idle": "2025-01-03T22:54:00.713926Z",
     "shell.execute_reply": "2025-01-03T22:54:00.712829Z",
     "shell.execute_reply.started": "2025-01-03T22:36:23.420234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 84.3MB/s]\n",
      "<ipython-input-6-1207a786f729>:51: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  attributes.iloc[:, 1:] = attributes.iloc[:, 1:].applymap(lambda x: 1 if x == 1 else 0)\n",
      "<ipython-input-6-1207a786f729>:51: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  attributes.iloc[:, 1:] = attributes.iloc[:, 1:].applymap(lambda x: 1 if x == 1 else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5087/5087 [05:44<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.3825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 621/621 [00:37<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 57.65%\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5087/5087 [03:21<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 621/621 [00:20<00:00, 30.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 58.43%\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5087/5087 [03:21<00:00, 25.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 621/621 [00:20<00:00, 30.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 58.75%\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5087/5087 [03:21<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 621/621 [00:20<00:00, 30.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 58.57%\n",
      "Training and evaluation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Path to CelebA images and metadata\n",
    "celeba_images_path = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "partition_file = \"/kaggle/input/celeba-dataset/list_eval_partition.csv\"\n",
    "attributes_file = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\n",
    "\n",
    "# Function to get DataLoader for CelebA\n",
    "def get_dataloader(\n",
    "        batch_size=128, num_workers=4, split=\"train\", shuffle=False, augment=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Get DataLoader for the CelebA dataset.\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    if augment:\n",
    "        transforms_pipeline = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.CenterCrop(178),  # Crop central face region\n",
    "                transforms.Resize(128),  # Resize to smaller dimensions\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalize to [-1, 1]\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transforms_pipeline = transforms.Compose(\n",
    "            [\n",
    "                transforms.CenterCrop(178),\n",
    "                transforms.Resize(128),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Determine dataset split\n",
    "    dataset_split = \"train\" if split == \"train\" else \"valid\"\n",
    "\n",
    "    # Load partition and attributes\n",
    "    partitions = pd.read_csv(partition_file)\n",
    "    attributes = pd.read_csv(attributes_file)\n",
    "\n",
    "    # Ensure attributes are binary\n",
    "    attributes.iloc[:, 1:] = attributes.iloc[:, 1:].applymap(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    # Select indices based on split\n",
    "    if dataset_split == \"train\":\n",
    "        selected_indices = partitions[partitions['partition'] == 0].index\n",
    "    else:\n",
    "        selected_indices = partitions[partitions['partition'] == 1].index\n",
    "\n",
    "    # Custom Dataset class for CelebA\n",
    "    class CelebADataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, indices, img_dir, attributes, transform=None):\n",
    "            self.indices = indices\n",
    "            self.img_dir = img_dir\n",
    "            self.attributes = attributes\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_index = self.indices[idx]\n",
    "            img_name = self.attributes.iloc[img_index, 0]  # Image file name\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # Get class label (convert to long tensor)\n",
    "            label = torch.tensor(self.attributes.iloc[img_index, 1:].values.astype('float32'))\n",
    "            label = label.argmax().long()\n",
    "            return image, label\n",
    "\n",
    "    # Create Dataset and DataLoader\n",
    "    dataset = CelebADataset(\n",
    "        indices=selected_indices,\n",
    "        img_dir=celeba_images_path,\n",
    "        attributes=attributes,\n",
    "        transform=transforms_pipeline\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset, shuffle=shuffle, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return loader, dataset\n",
    "\n",
    "# Load pre-trained model\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model_before_mitigating = resnet18(weights=ResNet18_Weights.DEFAULT).cuda()\n",
    "model_before_mitigating.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_before_mitigating.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Get DataLoaders\n",
    "train_loader, train_dataset = get_dataloader(batch_size=32, split=\"train\")\n",
    "val_loader, val_dataset = get_dataloader(batch_size=32, split=\"val\", shuffle=False, augment=False)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 4  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    model_before_mitigating.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_before_mitigating(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model_before_mitigating.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model_before_mitigating(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Final Output\n",
    "print(\"Training and evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T22:54:03.686175Z",
     "iopub.status.busy": "2025-01-03T22:54:03.685872Z",
     "iopub.status.idle": "2025-01-03T22:54:06.687382Z",
     "shell.execute_reply": "2025-01-03T22:54:06.686648Z",
     "shell.execute_reply.started": "2025-01-03T22:54:03.686151Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Distribution:\n",
      "subgroup\n",
      "young-female    103287\n",
      "young-male       53447\n",
      "old-male         30987\n",
      "old-female       14878\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CelebA attributes file\n",
    "attributes_file = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\n",
    "attributes = pd.read_csv(attributes_file)\n",
    "\n",
    "attributes['Young'] = attributes['Young'].apply(lambda x: 1 if x == 1 else 0)\n",
    "attributes['Male'] = attributes['Male'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "def define_subgroups(row):\n",
    "    if row['Young'] == 1 and row['Male'] == 1:\n",
    "        return 'young-male'\n",
    "    elif row['Young'] == 1 and row['Male'] == 0:\n",
    "        return 'young-female'\n",
    "    elif row['Young'] == 0 and row['Male'] == 1:\n",
    "        return 'old-male'\n",
    "    elif row['Young'] == 0 and row['Male'] == 0:\n",
    "        return 'old-female'\n",
    "\n",
    "attributes['subgroup'] = attributes.apply(define_subgroups, axis=1)\n",
    "\n",
    "subgroup_mapping = {name: i for i, name in enumerate(attributes['subgroup'].unique())}\n",
    "attributes['group_index'] = attributes['subgroup'].map(subgroup_mapping)\n",
    "\n",
    "print(\"Subgroup Distribution:\")\n",
    "print(attributes['subgroup'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T22:54:16.162863Z",
     "iopub.status.busy": "2025-01-03T22:54:16.162564Z",
     "iopub.status.idle": "2025-01-03T22:54:16.168006Z",
     "shell.execute_reply": "2025-01-03T22:54:16.167282Z",
     "shell.execute_reply.started": "2025-01-03T22:54:16.162841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "group_labels = attributes.loc[val_dataset.indices, 'group_index'].values\n",
    "# group_inds = group_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T20:39:21.459822Z",
     "iopub.status.busy": "2025-01-04T20:39:21.459531Z",
     "iopub.status.idle": "2025-01-04T20:39:21.467294Z",
     "shell.execute_reply": "2025-01-04T20:39:21.466325Z",
     "shell.execute_reply.started": "2025-01-04T20:39:21.459800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_worst_group_accuracy(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "    group_labels = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating WGA\")):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` since labels are not one-hot encoded\n",
    "\n",
    "            # Predict using the model\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Assign predictions and labels to the respective group\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])  # Add single prediction\n",
    "                group_labels[group].append(labels.cpu().numpy()[i])  # Add single label\n",
    "\n",
    "    group_accuracies = {}\n",
    "    for group in group_preds.keys():\n",
    "        if len(group_preds[group]) == 0 or len(group_labels[group]) == 0:\n",
    "            # Skip groups with no samples\n",
    "            group_accuracies[group] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Convert lists to arrays\n",
    "        preds = np.array(group_preds[group])\n",
    "        truths = np.array(group_labels[group])\n",
    "        group_accuracies[group] = accuracy_score(truths, preds)\n",
    "\n",
    "    # Print all group accuracies\n",
    "    for group, acc in group_accuracies.items():\n",
    "        print(f\"Group {group} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Find the worst group accuracy\n",
    "    worst_group_accuracy = min(group_accuracies.values())\n",
    "    return worst_group_accuracy, group_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Fairness Metrics for Young and Old Groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T22:54:24.097018Z",
     "iopub.status.busy": "2025-01-03T22:54:24.096409Z",
     "iopub.status.idle": "2025-01-03T22:54:24.113439Z",
     "shell.execute_reply": "2025-01-03T22:54:24.112539Z",
     "shell.execute_reply.started": "2025-01-03T22:54:24.096991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate Group Accuracies\n",
    "def evaluate_group_accuracies(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_preds = {g: [] for g in set(group_labels)}\n",
    "    group_truths = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating Group Accuracies\")):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` here\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Assign predictions and truths to respective groups\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[j])\n",
    "                group_truths[group].append(labels.cpu().numpy()[j])\n",
    "\n",
    "    group_accuracies = {}\n",
    "    for group in group_preds:\n",
    "        if len(group_preds[group]) == 0:\n",
    "            group_accuracies[group] = 0.0\n",
    "        else:\n",
    "            preds = np.array(group_preds[group])\n",
    "            truths = np.array(group_truths[group])\n",
    "            group_accuracies[group] = accuracy_score(truths, preds)\n",
    "\n",
    "    # Print group accuracies\n",
    "    for group, acc in group_accuracies.items():\n",
    "        print(f\"Group {group} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    return group_accuracies\n",
    "\n",
    "# Function to evaluate Demographic Parity (DP)\n",
    "def evaluate_demographic_parity(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_pprs = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(tqdm(val_loader, desc=\"Evaluating Demographic Parity\")):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(preds)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                group_pprs[group].append(preds[j])\n",
    "\n",
    "    ppr_disparities = {}\n",
    "    for group in group_pprs:\n",
    "        group_positive_rate = np.mean(group_pprs[group])\n",
    "        ppr_disparities[group] = group_positive_rate\n",
    "\n",
    "    # Print group PPRs\n",
    "    for group, ppr in ppr_disparities.items():\n",
    "        print(f\"Group {group} PPR: {ppr:.4f}\")\n",
    "    \n",
    "    return ppr_disparities\n",
    "\n",
    "# Function to evaluate Equal Opportunity (EO)\n",
    "def evaluate_equal_opportunity(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_tprs = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating Equal Opportunity\")):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` here\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                tp = (preds[j] == 1 and labels[j].cpu().numpy() == 1)\n",
    "                actual_positive = labels[j].cpu().numpy() == 1\n",
    "                group_tprs[group].append(tp / (actual_positive + 1e-8))  # Avoid division by zero\n",
    "\n",
    "    tpr_disparities = {}\n",
    "    for group in group_tprs:\n",
    "        tpr_disparities[group] = np.mean(group_tprs[group])\n",
    "\n",
    "    # Print group TPRs\n",
    "    for group, tpr in tpr_disparities.items():\n",
    "        print(f\"Group {group} TPR: {tpr:.4f}\")\n",
    "    \n",
    "    return tpr_disparities\n",
    "\n",
    "# Function to evaluate Equalized Odds (EOd)\n",
    "def evaluate_equalized_odds(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_tprs = {g: [] for g in set(group_labels)}\n",
    "    group_fprs = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating Equalized Odds\")):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` here\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                tp = (preds[j] == 1 and labels[j].cpu().numpy() == 1)\n",
    "                fp = (preds[j] == 1 and labels[j].cpu().numpy() == 0)\n",
    "                actual_positive = labels[j].cpu().numpy() == 1\n",
    "                actual_negative = labels[j].cpu().numpy() == 0\n",
    "\n",
    "                group_tprs[group].append(tp / (actual_positive + 1e-8))  # Avoid division by zero\n",
    "                group_fprs[group].append(fp / (actual_negative + 1e-8))  # Avoid division by zero\n",
    "\n",
    "    tpr_disparities = {}\n",
    "    fpr_disparities = {}\n",
    "    for group in group_tprs:\n",
    "        tpr_disparities[group] = np.mean(group_tprs[group])\n",
    "        fpr_disparities[group] = np.mean(group_fprs[group])\n",
    "\n",
    "    # Print group TPRs and FPRs\n",
    "    for group in group_tprs:\n",
    "        print(f\"Group {group} TPR: {tpr_disparities[group]:.4f}, FPR: {fpr_disparities[group]:.4f}\")\n",
    "    \n",
    "    return tpr_disparities, fpr_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T22:54:31.197444Z",
     "iopub.status.busy": "2025-01-03T22:54:31.197123Z",
     "iopub.status.idle": "2025-01-03T22:55:47.039303Z",
     "shell.execute_reply": "2025-01-03T22:55:47.038224Z",
     "shell.execute_reply.started": "2025-01-03T22:54:31.197416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 621/621 [00:20<00:00, 30.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 Accuracy: 0.6536\n",
      "Group 1 Accuracy: 0.5402\n",
      "Group 2 Accuracy: 0.4719\n",
      "Group 3 Accuracy: 0.5593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Demographic Parity: 100%|██████████| 621/621 [00:18<00:00, 33.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 PPR: 2.0323\n",
      "Group 1 PPR: 2.7719\n",
      "Group 2 PPR: 4.5824\n",
      "Group 3 PPR: 2.8930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Equal Opportunity: 100%|██████████| 621/621 [00:18<00:00, 34.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 TPR: 0.3203\n",
      "Group 1 TPR: 0.0146\n",
      "Group 2 TPR: 0.0090\n",
      "Group 3 TPR: 0.3253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Equalized Odds: 100%|██████████| 621/621 [00:19<00:00, 32.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 TPR: 0.3203, FPR: 0.0001\n",
      "Group 1 TPR: 0.0146, FPR: 0.0033\n",
      "Group 2 TPR: 0.0090, FPR: 0.0003\n",
      "Group 3 TPR: 0.3253, FPR: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wga, group_accuracies = evaluate_worst_group_accuracy(model_before_mitigating, val_loader, group_labels)\n",
    "dp_rates = evaluate_demographic_parity(model_before_mitigating, val_loader, group_labels)\n",
    "eo_tprs = evaluate_equal_opportunity(model_before_mitigating, val_loader, group_labels)\n",
    "tpr_disparities, fpr_disparities = evaluate_equalized_odds(model_before_mitigating, val_loader, group_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2025-01-02T20:47:42.388294Z",
     "iopub.status.idle": "2025-01-02T20:47:42.388652Z",
     "shell.execute_reply": "2025-01-02T20:47:42.388499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_dataloader(\n",
    "        batch_size=128, num_workers=5, split=\"train\", shuffle=False, augment=True\n",
    "    ):\n",
    "        if augment:\n",
    "            transforms = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.RandomHorizontalFlip(),\n",
    "                    torchvision.transforms.RandomAffine(0),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(\n",
    "                        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.201)\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transforms = torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(\n",
    "                        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.201)\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        is_train = split == \"train\"\n",
    "        dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"/tmp/cifar/\", download=True, train=is_train, transform=transforms\n",
    "        )\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset=dataset, shuffle=shuffle, batch_size=batch_size, num_workers=num_workers\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "# Load pre-trained model\n",
    "model_before_mitigating = torchvision.models.resnet18(pretrained=True).cuda()\n",
    "model_before_mitigating.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define loss function and optimizer (if training)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_before_mitigating.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Get data loaders\n",
    "train_loader = get_dataloader(batch_size=32, split=\"train\")\n",
    "val_loader = get_dataloader(batch_size=32, split=\"val\", shuffle=False, augment=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model_before_mitigating.train()  # Set model to training mode\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_before_mitigating(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model_before_mitigating.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model_before_mitigating(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "# model_before_mitigating = torchvision.models.resnet18(pretrained=True).cuda().eval()\n",
    "\n",
    "# #  put some random init weights as a placeholder\n",
    "# ckpts = [model_before_mitigating.state_dict()]\n",
    "\n",
    "# train_loader = get_dataloader(batch_size=64)\n",
    "# val_loader = get_dataloader(split=\"val\", batch_size=64)\n",
    "\n",
    "#  random group allocations as a placeholder\n",
    "ckpts = [model_before_mitigating.state_dict()]\n",
    "group_inds = [np.random.choice(10) for i in range(len(val_loader.dataset))]\n",
    "# print(f'ckpts; {ckpts}')\n",
    "# print(f'group indicis; {group_inds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:11:32.120964Z",
     "iopub.status.busy": "2025-01-04T21:11:32.120612Z",
     "iopub.status.idle": "2025-01-04T21:11:58.155335Z",
     "shell.execute_reply": "2025-01-04T21:11:58.154456Z",
     "shell.execute_reply.started": "2025-01-04T21:11:32.120934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.3058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy: 93.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy: 94.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy: 93.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy: 94.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy: 94.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy: 93.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy: 93.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy: 93.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy: 93.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Validation Accuracy: 93.69%\n",
      "Training and evaluation completed.\n",
      "Group Indices (Sample): [2. 0. 0. 0. 2. 2. 1. 2. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load COMPAS dataset\n",
    "compas_data_path = \"/kaggle/input/compas-scores-two-years/compas-scores-two-years.csv\"  # Replace with actual path\n",
    "df = pd.read_csv(compas_data_path)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "columns_to_drop = [\n",
    "    \"name\", \"compas_screening_date\", \"dob\", \"c_case_number\",  # Personal identifiers\n",
    "    \"is_violent_recid\", \"is_recid\", \"c_charge_desc\"           # Other non-numeric data\n",
    "]\n",
    "df = df.drop(columns=columns_to_drop, errors=\"ignore\")  # Ignore missing columns\n",
    "\n",
    "# Separate features and labels\n",
    "labels = df[\"two_year_recid\"]  # Target variable\n",
    "features = df.drop(columns=[\"two_year_recid\"], errors=\"ignore\")  # Drop target column from features\n",
    "\n",
    "# Filter numerical features without missing values\n",
    "numerical_features = features.select_dtypes(include=[np.number]).dropna(axis=1)\n",
    "\n",
    "# Encode categorical features (e.g., race, sex)\n",
    "categorical_features = features.select_dtypes(include=[\"object\", \"category\"])\n",
    "categorical_features = pd.get_dummies(categorical_features, drop_first=True)\n",
    "\n",
    "# Combine processed numerical and categorical features\n",
    "features = pd.concat([numerical_features, categorical_features], axis=1)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define COMPAS Dataset Class\n",
    "class COMPASDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# Define DataLoader Function\n",
    "def get_compas_dataloader(batch_size=128, split=\"train\", shuffle=False):\n",
    "    \"\"\"\n",
    "    Get DataLoader for the COMPAS dataset.\n",
    "    \"\"\"\n",
    "    if split == \"train\":\n",
    "        dataset = COMPASDataset(X_train, y_train.values)\n",
    "    else:\n",
    "        dataset = COMPASDataset(X_val, y_val.values)\n",
    "\n",
    "    # DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return loader, len(dataset)\n",
    "\n",
    "# Load simplified model (for tabular data)\n",
    "class COMPASModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(COMPASModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)  # Binary classification (2 classes: reoffended or not)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define dataset and model\n",
    "train_loader, train_size = get_compas_dataloader(batch_size=32, split=\"train\", shuffle=True)\n",
    "val_loader, val_size = get_compas_dataloader(batch_size=32, split=\"val\", shuffle=False)\n",
    "\n",
    "# Input size depends on processed feature count\n",
    "input_size = next(iter(train_loader))[0].shape[1]\n",
    "model_before_mitigating = COMPASModel(input_size).cuda()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_before_mitigating.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_before_mitigating.train()\n",
    "    epoch_loss = 0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False) as pbar:\n",
    "        for inputs, labels in pbar:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_before_mitigating(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_description(f\"Epoch {epoch+1} Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model_before_mitigating.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(), tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False) as pbar:\n",
    "        for inputs, labels in pbar:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model_before_mitigating(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            pbar.set_description(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Define group indices based on race\n",
    "race_mapping = {\"African-American\": 0, \"Caucasian\": 1, \"Other\": 2}\n",
    "group_inds = df[\"race\"].map(race_mapping).fillna(2).values  # Default to 'Other' if missing\n",
    "group_inds = group_inds[:len(X_val)]  # Align with validation dataset size\n",
    "\n",
    "# Print message to confirm completion\n",
    "print(\"Training and evaluation completed.\")\n",
    "print(f\"Group Indices (Sample): {group_inds[:10]}\")  # Print sample group indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:06:50.497303Z",
     "iopub.status.busy": "2025-01-04T21:06:50.496937Z",
     "iopub.status.idle": "2025-01-04T21:08:01.532751Z",
     "shell.execute_reply": "2025-01-04T21:08:01.531941Z",
     "shell.execute_reply.started": "2025-01-04T21:06:50.497273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOYO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing features for all model IDs..: 100%|██████████| 1/1 [00:00<00:00, 6384.02it/s]\n",
      "Finalizing scores for all model IDs..: 100%|██████████| 1/1 [00:00<00:00, 40.46it/s]\n"
     ]
    }
   ],
   "source": [
    "print('YOYO')\n",
    "ckpts = [model_before_mitigating.state_dict()]\n",
    "dda = DDA(model_before_mitigating, ckpts, train_loader, val_loader, group_inds)\n",
    "\n",
    "# debiased_inds = dda.debias()\n",
    "# print(dda.trak_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:31.329527Z",
     "iopub.status.busy": "2025-01-04T21:08:31.329218Z",
     "iopub.status.idle": "2025-01-04T21:08:31.528111Z",
     "shell.execute_reply": "2025-01-04T21:08:31.527214Z",
     "shell.execute_reply.started": "2025-01-04T21:08:31.329503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2882"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debiased_inds = dda.debias(use_heuristic=False, num_to_discard=400)\n",
    "debiased_inds = dda.debias(use_heuristic=True)\n",
    "len(debiased_inds)\n",
    "# debiased_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:34.459615Z",
     "iopub.status.busy": "2025-01-04T21:08:34.459337Z",
     "iopub.status.idle": "2025-01-04T21:08:34.599568Z",
     "shell.execute_reply": "2025-01-04T21:08:34.598786Z",
     "shell.execute_reply.started": "2025-01-04T21:08:34.459594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Overall Accuracy for the first model...\n",
      "Overall Accuracy: 0.9390\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "\n",
    "def evaluate_overall_accuracy(model, val_loader, device=\"cuda\"):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Predict using the model\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Collect all predictions and labels\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Evaluating Overall Accuracy for the first model...\")\n",
    "overall_accuracy = evaluate_overall_accuracy(model_before_mitigating, val_loader, device=device)\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate WGA before intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:38.030970Z",
     "iopub.status.busy": "2025-01-04T21:08:38.030661Z",
     "iopub.status.idle": "2025-01-04T21:08:38.209398Z",
     "shell.execute_reply": "2025-01-04T21:08:38.208454Z",
     "shell.execute_reply.started": "2025-01-04T21:08:38.030944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Distribution: {0.0: 728, 1.0: 485, 2.0: 230}\n",
      "Evaluating Worst Group Accuracy (WGA) for the first model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 278.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.9368\n",
      "Group 1.0 Accuracy: 0.9443\n",
      "Group 2.0 Accuracy: 0.9348\n",
      "Group Accuracies: {0.0: 0.9368131868131868, 1.0: 0.9443298969072165, 2.0: 0.9347826086956522}\n",
      "Worst Group Accuracy: 0.9348\n",
      "Difference between Worst and Best Group: 0.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# group_inds = [np.random.choice(10) for i in range(len(val_loader.dataset))]\n",
    "# Check alignment of group_inds and dataset\n",
    "assert len(group_inds) == len(val_loader.dataset), \"Group indices length mismatch.\"\n",
    "print(f\"Group Distribution: {dict(zip(*np.unique(group_inds, return_counts=True)))}\")\n",
    "\n",
    "# Evaluate Worst Group Accuracy\n",
    "print(\"Evaluating Worst Group Accuracy (WGA) for the first model...\")\n",
    "worst_group_acc, group_accuracies = evaluate_worst_group_accuracy(model_before_mitigating, val_loader, group_inds, device=\"cuda\")\n",
    "\n",
    "print(f\"Group Accuracies: {group_accuracies}\")\n",
    "print(f\"Worst Group Accuracy: {worst_group_acc:.4f}\")\n",
    "print(f\"Difference between Worst and Best Group: {(max(group_accuracies.values()) - worst_group_acc):.4f}\")\n",
    "\n",
    "import copy\n",
    "\n",
    "deep_copy_model = copy.deepcopy(model_before_mitigating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:47.451636Z",
     "iopub.status.busy": "2025-01-04T21:08:47.451337Z",
     "iopub.status.idle": "2025-01-04T21:08:47.458463Z",
     "shell.execute_reply": "2025-01-04T21:08:47.457663Z",
     "shell.execute_reply.started": "2025-01-04T21:08:47.451612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_tpr(labels, preds):\n",
    "    tp = np.sum((preds == 1) & (labels == 1))\n",
    "    fn = np.sum((preds == 0) & (labels == 1))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "def evaluate_equal_opportunity(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "    group_labels = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])\n",
    "                group_labels[group].append(labels.cpu().numpy()[i])\n",
    "\n",
    "    group_tprs = {}\n",
    "    for group in group_preds.keys():\n",
    "        preds = np.array(group_preds[group])\n",
    "        labels = np.array(group_labels[group])\n",
    "        group_tprs[group] = calculate_tpr(labels, preds)\n",
    "\n",
    "    min_tpr = min(group_tprs.values())\n",
    "    max_tpr = max(group_tprs.values())\n",
    "    tpr_disparity = max_tpr - min_tpr\n",
    "\n",
    "    return group_tprs, tpr_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:49.370758Z",
     "iopub.status.busy": "2025-01-04T21:08:49.370475Z",
     "iopub.status.idle": "2025-01-04T21:08:49.538616Z",
     "shell.execute_reply": "2025-01-04T21:08:49.537695Z",
     "shell.execute_reply.started": "2025-01-04T21:08:49.370735Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group TPRs: {0.0: 0.9013157894736842, 1.0: 0.8986175115207373, 2.0: 0.898989898989899}\n",
      "TPR Disparity: 0.0027\n"
     ]
    }
   ],
   "source": [
    "group_tprs, tpr_disparity = evaluate_equal_opportunity(deep_copy_model, val_loader, group_inds)\n",
    "print(f\"Group TPRs: {group_tprs}\")\n",
    "print(f\"TPR Disparity: {tpr_disparity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:51.943110Z",
     "iopub.status.busy": "2025-01-04T21:08:51.942808Z",
     "iopub.status.idle": "2025-01-04T21:08:51.950520Z",
     "shell.execute_reply": "2025-01-04T21:08:51.949566Z",
     "shell.execute_reply.started": "2025-01-04T21:08:51.943088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_fpr(labels, preds):\n",
    "    fp = np.sum((preds == 1) & (labels == 0))\n",
    "    tn = np.sum((preds == 0) & (labels == 0))\n",
    "    return fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "def evaluate_equalized_odds(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "    group_labels = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])\n",
    "                group_labels[group].append(labels.cpu().numpy()[i])\n",
    "\n",
    "    group_tprs, group_fprs = {}, {}\n",
    "    for group in group_preds.keys():\n",
    "        preds = np.array(group_preds[group])\n",
    "        labels = np.array(group_labels[group])\n",
    "        group_tprs[group] = calculate_tpr(labels, preds)\n",
    "        group_fprs[group] = calculate_fpr(labels, preds)\n",
    "\n",
    "    tpr_disparity = max(group_tprs.values()) - min(group_tprs.values())\n",
    "    fpr_disparity = max(group_fprs.values()) - min(group_fprs.values())\n",
    "\n",
    "    return group_tprs, group_fprs, tpr_disparity, fpr_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:54.083996Z",
     "iopub.status.busy": "2025-01-04T21:08:54.083677Z",
     "iopub.status.idle": "2025-01-04T21:08:54.258662Z",
     "shell.execute_reply": "2025-01-04T21:08:54.257856Z",
     "shell.execute_reply.started": "2025-01-04T21:08:54.083969Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group TPRs: {0.0: 0.9013157894736842, 1.0: 0.8986175115207373, 2.0: 0.898989898989899}\n",
      "Group FPRs: {0.0: 0.03773584905660377, 1.0: 0.018656716417910446, 2.0: 0.03816793893129771}\n",
      "TPR Disparity: 0.0027\n",
      "FPR Disparity: 0.0195\n"
     ]
    }
   ],
   "source": [
    "group_tprs, group_fprs, tpr_disparity, fpr_disparity = evaluate_equalized_odds(deep_copy_model, val_loader, group_inds)\n",
    "print(f\"Group TPRs: {group_tprs}\")\n",
    "print(f\"Group FPRs: {group_fprs}\")\n",
    "print(f\"TPR Disparity: {tpr_disparity:.4f}\")\n",
    "print(f\"FPR Disparity: {fpr_disparity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:08:58.602359Z",
     "iopub.status.busy": "2025-01-04T21:08:58.602054Z",
     "iopub.status.idle": "2025-01-04T21:08:58.608674Z",
     "shell.execute_reply": "2025-01-04T21:08:58.607796Z",
     "shell.execute_reply.started": "2025-01-04T21:08:58.602334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_ppr(preds):\n",
    "    return np.mean(preds)\n",
    "\n",
    "\n",
    "def evaluate_demographic_parity(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate Demographic Parity.\n",
    "    Ensures PPR is correctly normalized as probabilities.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Get the group indices for the current batch\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            # Assign predictions to the corresponding group\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])\n",
    "\n",
    "    group_pprs = {}\n",
    "    for group in group_preds.keys():\n",
    "        # Flatten the predictions list for each group and normalize\n",
    "        preds = np.array(group_preds[group]).flatten()\n",
    "        positive_preds = (preds == 1).sum()  # Count positive predictions\n",
    "        total_preds = len(preds)  # Total number of predictions\n",
    "        group_pprs[group] = positive_preds / total_preds if total_preds > 0 else 0.0\n",
    "\n",
    "    # Calculate the disparity in PPRs across groups\n",
    "    min_ppr = min(group_pprs.values())\n",
    "    max_ppr = max(group_pprs.values())\n",
    "    ppr_disparity = max_ppr - min_ppr\n",
    "\n",
    "    return group_pprs, ppr_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:09:00.526662Z",
     "iopub.status.busy": "2025-01-04T21:09:00.526359Z",
     "iopub.status.idle": "2025-01-04T21:09:00.673358Z",
     "shell.execute_reply": "2025-01-04T21:09:00.672386Z",
     "shell.execute_reply.started": "2025-01-04T21:09:00.526639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group PPRs: {0.0: 0.3983516483516483, 1.0: 0.41237113402061853, 2.0: 0.40869565217391307}\n",
      "PPR Disparity: 0.0140\n"
     ]
    }
   ],
   "source": [
    "group_pprs, ppr_disparity = evaluate_demographic_parity(deep_copy_model, val_loader, group_inds)\n",
    "print(f\"Group PPRs: {group_pprs}\")\n",
    "print(f\"PPR Disparity: {ppr_disparity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T21:09:03.330777Z",
     "iopub.status.busy": "2025-01-04T21:09:03.330463Z",
     "iopub.status.idle": "2025-01-04T21:09:03.487764Z",
     "shell.execute_reply": "2025-01-04T21:09:03.486866Z",
     "shell.execute_reply.started": "2025-01-04T21:09:03.330745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating FNR and FPR: 100%|██████████| 46/46 [00:00<00:00, 323.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group FNR and FPR:\n",
      "Group 0.0: FNR = 0.0987, FPR = 0.0377\n",
      "Group 1.0: FNR = 0.1014, FPR = 0.0187\n",
      "Group 2.0: FNR = 0.1010, FPR = 0.0382\n",
      "\n",
      "FNR Disparity (Max - Min): 0.0027\n",
      "FPR Disparity (Max - Min): 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_fnr_fpr(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Calculate False Negative Rate (FNR) and False Positive Rate (FPR) for each group.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    group_metrics = {g: {\"FN\": 0, \"FP\": 0, \"TP\": 0, \"TN\": 0} for g in set(group_inds)}  # Metrics for each group\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(val_loader, desc=\"Calculating FNR and FPR\")):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            # Get the groups for the current batch\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                if group not in group_metrics:\n",
    "                    continue  # Skip if group is not defined\n",
    "                \n",
    "                # Update confusion matrix components\n",
    "                if labels[i] == 1 and preds[i] == 0:  # False Negative\n",
    "                    group_metrics[group][\"FN\"] += 1\n",
    "                elif labels[i] == 0 and preds[i] == 1:  # False Positive\n",
    "                    group_metrics[group][\"FP\"] += 1\n",
    "                elif labels[i] == 1 and preds[i] == 1:  # True Positive\n",
    "                    group_metrics[group][\"TP\"] += 1\n",
    "                elif labels[i] == 0 and preds[i] == 0:  # True Negative\n",
    "                    group_metrics[group][\"TN\"] += 1\n",
    "\n",
    "    # Calculate FNR and FPR for each group\n",
    "    group_fnr_fpr = {}\n",
    "    for group, metrics in group_metrics.items():\n",
    "        fn = metrics[\"FN\"]\n",
    "        fp = metrics[\"FP\"]\n",
    "        tp = metrics[\"TP\"]\n",
    "        tn = metrics[\"TN\"]\n",
    "\n",
    "        actual_positives = tp + fn\n",
    "        actual_negatives = tn + fp\n",
    "\n",
    "        fnr = fn / (actual_positives + 1e-8) if actual_positives > 0 else 0.0\n",
    "        fpr = fp / (actual_negatives + 1e-8) if actual_negatives > 0 else 0.0\n",
    "\n",
    "        group_fnr_fpr[group] = {\"FNR\": fnr, \"FPR\": fpr}\n",
    "\n",
    "    # Print FNR and FPR for each group\n",
    "    print(\"\\nGroup FNR and FPR:\")\n",
    "    for group, metrics in group_fnr_fpr.items():\n",
    "        print(f\"Group {group}: FNR = {metrics['FNR']:.4f}, FPR = {metrics['FPR']:.4f}\")\n",
    "\n",
    "    # Calculate and print disparities\n",
    "    fnr_values = [metrics[\"FNR\"] for metrics in group_fnr_fpr.values()]\n",
    "    fpr_values = [metrics[\"FPR\"] for metrics in group_fnr_fpr.values()]\n",
    "    fnr_disparity = max(fnr_values) - min(fnr_values)\n",
    "    fpr_disparity = max(fpr_values) - min(fpr_values)\n",
    "\n",
    "    print(f\"\\nFNR Disparity (Max - Min): {fnr_disparity:.4f}\")\n",
    "    print(f\"FPR Disparity (Max - Min): {fpr_disparity:.4f}\")\n",
    "\n",
    "    return group_fnr_fpr, fnr_disparity, fpr_disparity\n",
    "\n",
    "\n",
    "# Example usage\n",
    "group_fnr_fpr, fnr_disparity, fpr_disparity = calculate_fnr_fpr(model_before_mitigating, val_loader, group_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T20:43:15.730961Z",
     "iopub.status.busy": "2025-01-04T20:43:15.730681Z",
     "iopub.status.idle": "2025-01-04T20:45:20.139691Z",
     "shell.execute_reply": "2025-01-04T20:45:20.138847Z",
     "shell.execute_reply.started": "2025-01-04T20:43:15.730940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 697.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7047\n",
      "Group 1.0 Accuracy: 0.6680\n",
      "Group 2.0 Accuracy: 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 699.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7033\n",
      "Group 1.0 Accuracy: 0.6742\n",
      "Group 2.0 Accuracy: 0.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 694.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7005\n",
      "Group 1.0 Accuracy: 0.6784\n",
      "Group 2.0 Accuracy: 0.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 680.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6964\n",
      "Group 1.0 Accuracy: 0.6784\n",
      "Group 2.0 Accuracy: 0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 695.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7019\n",
      "Group 1.0 Accuracy: 0.6742\n",
      "Group 2.0 Accuracy: 0.6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 703.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7033\n",
      "Group 1.0 Accuracy: 0.6742\n",
      "Group 2.0 Accuracy: 0.6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 694.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7033\n",
      "Group 1.0 Accuracy: 0.6722\n",
      "Group 2.0 Accuracy: 0.6783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 681.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6992\n",
      "Group 1.0 Accuracy: 0.6680\n",
      "Group 2.0 Accuracy: 0.6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 681.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6978\n",
      "Group 1.0 Accuracy: 0.6742\n",
      "Group 2.0 Accuracy: 0.6783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 681.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7019\n",
      "Group 1.0 Accuracy: 0.6680\n",
      "Group 2.0 Accuracy: 0.6739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 687.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7033\n",
      "Group 1.0 Accuracy: 0.6660\n",
      "Group 2.0 Accuracy: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 691.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7019\n",
      "Group 1.0 Accuracy: 0.6619\n",
      "Group 2.0 Accuracy: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 696.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6992\n",
      "Group 1.0 Accuracy: 0.6680\n",
      "Group 2.0 Accuracy: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 701.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6964\n",
      "Group 1.0 Accuracy: 0.6557\n",
      "Group 2.0 Accuracy: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 673.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6992\n",
      "Group 1.0 Accuracy: 0.6557\n",
      "Group 2.0 Accuracy: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 698.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6964\n",
      "Group 1.0 Accuracy: 0.6557\n",
      "Group 2.0 Accuracy: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 703.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6964\n",
      "Group 1.0 Accuracy: 0.6536\n",
      "Group 2.0 Accuracy: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 701.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6937\n",
      "Group 1.0 Accuracy: 0.6495\n",
      "Group 2.0 Accuracy: 0.6565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 705.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.6978\n",
      "Group 1.0 Accuracy: 0.6495\n",
      "Group 2.0 Accuracy: 0.6565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 46/46 [00:00<00:00, 706.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0.0 Accuracy: 0.7005\n",
      "Group 1.0 Accuracy: 0.6598\n",
      "Group 2.0 Accuracy: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "harmful_indices = debiased_inds\n",
    "\n",
    "def remove_influence(model, dataloader, harmful_indices, factor, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    harmful_dataset = torch.utils.data.Subset(dataloader.dataset, harmful_indices)\n",
    "    harmful_loader = torch.utils.data.DataLoader(harmful_dataset, batch_size=1)\n",
    "\n",
    "    for inputs, labels in harmful_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                param -= grad * factor\n",
    "\n",
    "    return model\n",
    "\n",
    "results ={'factor':[], 'model':[], 'min':[], 'max':[], 'gap':[]}\n",
    "factors = np.linspace(0.0001, 0.01, 20)\n",
    "\n",
    "for factor in factors:\n",
    "    newdeepmodel = copy.deepcopy(deep_copy_model)\n",
    "    m = remove_influence(newdeepmodel, train_loader, harmful_indices, factor, device=\"cuda\")\n",
    "    wga, group_accs = evaluate_worst_group_accuracy(m, val_loader, group_inds, device=\"cuda\")\n",
    "    current_gap = (max(group_accs.values()) - wga)\n",
    "    results['model'].append(m)\n",
    "    results['min'].append(wga)\n",
    "    results['max'].append(max(group_accs.values()))\n",
    "    results['gap'].append(current_gap)\n",
    "    results['factor'].append(factor)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T20:45:20.140960Z",
     "iopub.status.busy": "2025-01-04T20:45:20.140693Z",
     "iopub.status.idle": "2025-01-04T20:45:20.154346Z",
     "shell.execute_reply": "2025-01-04T20:45:20.153542Z",
     "shell.execute_reply.started": "2025-01-04T20:45:20.140935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>model</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.704670</td>\n",
       "      <td>0.036629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000621</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.029070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001142</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.678351</td>\n",
       "      <td>0.700549</td>\n",
       "      <td>0.022199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001663</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.678351</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.018078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002184</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.701923</td>\n",
       "      <td>0.027696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002705</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.029070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003226</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.672165</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.031132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003747</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.699176</td>\n",
       "      <td>0.031135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004268</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.674227</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>0.023575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004789</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.668041</td>\n",
       "      <td>0.701923</td>\n",
       "      <td>0.033882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005311</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.665979</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.037317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005832</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.701923</td>\n",
       "      <td>0.041054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006353</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.665217</td>\n",
       "      <td>0.699176</td>\n",
       "      <td>0.033958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.006874</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.040758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007395</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.699176</td>\n",
       "      <td>0.043506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.007916</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.655670</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.040758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.008437</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.653608</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.042820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008958</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.649485</td>\n",
       "      <td>0.693681</td>\n",
       "      <td>0.044197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.009479</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.649485</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>0.048318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.700549</td>\n",
       "      <td>0.040756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      factor                                              model       min  \\\n",
       "0   0.000100  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.668041   \n",
       "1   0.000621  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.674227   \n",
       "2   0.001142  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.678351   \n",
       "3   0.001663  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.678351   \n",
       "4   0.002184  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.674227   \n",
       "5   0.002705  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.674227   \n",
       "6   0.003226  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.672165   \n",
       "7   0.003747  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.668041   \n",
       "8   0.004268  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.674227   \n",
       "9   0.004789  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.668041   \n",
       "10  0.005311  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.665979   \n",
       "11  0.005832  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.660870   \n",
       "12  0.006353  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.665217   \n",
       "13  0.006874  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.655670   \n",
       "14  0.007395  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.655670   \n",
       "15  0.007916  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.655670   \n",
       "16  0.008437  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.653608   \n",
       "17  0.008958  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.649485   \n",
       "18  0.009479  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.649485   \n",
       "19  0.010000  COMPASModel(\\n  (fc): Sequential(\\n    (0): Li...  0.659794   \n",
       "\n",
       "         max       gap  \n",
       "0   0.704670  0.036629  \n",
       "1   0.703297  0.029070  \n",
       "2   0.700549  0.022199  \n",
       "3   0.696429  0.018078  \n",
       "4   0.701923  0.027696  \n",
       "5   0.703297  0.029070  \n",
       "6   0.703297  0.031132  \n",
       "7   0.699176  0.031135  \n",
       "8   0.697802  0.023575  \n",
       "9   0.701923  0.033882  \n",
       "10  0.703297  0.037317  \n",
       "11  0.701923  0.041054  \n",
       "12  0.699176  0.033958  \n",
       "13  0.696429  0.040758  \n",
       "14  0.699176  0.043506  \n",
       "15  0.696429  0.040758  \n",
       "16  0.696429  0.042820  \n",
       "17  0.693681  0.044197  \n",
       "18  0.697802  0.048318  \n",
       "19  0.700549  0.040756  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results).sort_values('factor')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to investigate what are the best approaches to machine unlearning and how can we formulate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the other approaches to machine unlearning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Fairness notations might come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6424878,
     "sourceId": 10372207,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 167875528,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
