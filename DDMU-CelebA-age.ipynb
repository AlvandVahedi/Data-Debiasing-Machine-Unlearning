{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install traker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:35:33.828478Z",
     "iopub.status.busy": "2025-01-08T00:35:33.828213Z",
     "iopub.status.idle": "2025-01-08T00:35:37.368383Z",
     "shell.execute_reply": "2025-01-08T00:35:37.367507Z",
     "shell.execute_reply.started": "2025-01-08T00:35:33.828457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trak import TRAKer\n",
    "\n",
    "def get_trak_matrix(\n",
    "    train_dl, val_dl, model, ckpts, train_set_size, val_set_size, **kwargs\n",
    "):\n",
    "    if kwargs is None or kwargs.get(\"task\") is None:\n",
    "        task = \"image_classification\"\n",
    "    else:\n",
    "        task = kwargs.pop(\"task\")\n",
    "\n",
    "    traker = TRAKer(model=model, task=task, train_set_size=train_set_size, **kwargs)\n",
    "\n",
    "    for model_id, checkpoint in enumerate(ckpts):\n",
    "        traker.load_checkpoint(checkpoint, model_id=model_id)\n",
    "        for batch in train_dl:\n",
    "            batch = [x.cuda() for x in batch]\n",
    "            # batch should be a tuple/list of inputs and labels\n",
    "            traker.featurize(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "    traker.finalize_features()\n",
    "\n",
    "    for model_id, checkpoint in enumerate(ckpts):\n",
    "        traker.start_scoring_checkpoint(\n",
    "            exp_name=\"test\",\n",
    "            checkpoint=checkpoint,\n",
    "            model_id=model_id,\n",
    "            num_targets=val_set_size,\n",
    "        )\n",
    "    for batch in val_dl:\n",
    "        batch = [x.cuda() for x in batch]\n",
    "        traker.score(batch=batch, num_samples=batch[0].shape[0])\n",
    "\n",
    "    scores = traker.finalize_scores(exp_name=\"test\")\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:35:37.369677Z",
     "iopub.status.busy": "2025-01-08T00:35:37.369257Z",
     "iopub.status.idle": "2025-01-08T00:35:37.382015Z",
     "shell.execute_reply": "2025-01-08T00:35:37.381048Z",
     "shell.execute_reply.started": "2025-01-08T00:35:37.369645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class DDA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        checkpoints,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        group_indices,\n",
    "        train_set_size=None,\n",
    "        val_set_size=None,\n",
    "        trak_scores=None,\n",
    "        trak_kwargs=None,\n",
    "        device=\"cuda\",\n",
    "    ) -> None:\n",
    "        \n",
    "        self.model = model\n",
    "        self.checkpoints = checkpoints\n",
    "        self.dataloaders = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
    "        self.group_indices = group_indices\n",
    "        self.device = device\n",
    "\n",
    "        if trak_scores is not None:\n",
    "            self.trak_scores = trak_scores\n",
    "        else:\n",
    "            try:\n",
    "                self.train_set_size = len(train_dataloader.dataset)\n",
    "                self.val_set_size = len(val_dataloader.dataset)\n",
    "            except AttributeError as e:\n",
    "                print(\n",
    "                    f\"No dataset attribute found in train_dataloader or val_dataloader. {e}\"\n",
    "                )\n",
    "                if train_set_size is None or val_set_size is None:\n",
    "                    raise ValueError(\n",
    "                        \"train_set_size and val_set_size must be specified if \"\n",
    "                        \"train_dataloader and val_dataloader do not have a \"\n",
    "                        \"dataset attribute.\"\n",
    "                    ) from e\n",
    "                self.train_set_size = train_set_size\n",
    "                self.val_set_size = val_set_size\n",
    "\n",
    "            # Step 1: compute TRAK scores\n",
    "            if trak_kwargs is not None:\n",
    "                trak_scores = get_trak_matrix(\n",
    "                    train_dl=self.dataloaders[\"train\"],\n",
    "                    val_dl=self.dataloaders[\"val\"],\n",
    "                    model=self.model,\n",
    "                    ckpts=self.checkpoints,\n",
    "                    train_set_size=self.train_set_size,\n",
    "                    val_set_size=self.val_set_size,\n",
    "                    **trak_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                trak_scores = get_trak_matrix(\n",
    "                    train_dl=self.dataloaders[\"train\"],\n",
    "                    val_dl=self.dataloaders[\"val\"],\n",
    "                    model=self.model,\n",
    "                    ckpts=self.checkpoints,\n",
    "                    train_set_size=self.train_set_size,\n",
    "                    val_set_size=self.val_set_size,\n",
    "                )\n",
    "\n",
    "            self.trak_scores = trak_scores\n",
    "\n",
    "    def get_group_losses(self, model, val_dl, group_indices) -> list:\n",
    "        losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_dl:\n",
    "                outputs = model(inputs.to(self.device))\n",
    "                loss = F.cross_entropy(\n",
    "                    outputs, labels.to(self.device), reduction=\"none\"\n",
    "                )\n",
    "                losses.append(loss)\n",
    "        losses = torch.cat(losses)\n",
    "\n",
    "        n_groups = len(set(group_indices))\n",
    "        group_losses = [losses[group_indices == i].mean() for i in range(n_groups)]\n",
    "        return group_losses\n",
    "\n",
    "    def compute_group_alignment_scores(self, trak_scores, group_indices, group_losses):\n",
    "        n_groups = len(set(group_indices))\n",
    "        S = np.array(trak_scores)\n",
    "        g = [\n",
    "            group_losses[i].cpu().numpy() * S[:, np.array(group_indices) == i].mean(axis=1)\n",
    "            for i in range(n_groups)\n",
    "        ]\n",
    "        g = np.stack(g)\n",
    "        group_alignment_scores = g.mean(axis=0)\n",
    "        return group_alignment_scores\n",
    "\n",
    "    def get_debiased_train_indices(\n",
    "        self, group_alignment_scores, use_heuristic=True, num_to_discard=None\n",
    "    ):\n",
    "        if use_heuristic:\n",
    "            return [i for i, score in enumerate(group_alignment_scores) if score >= 0]\n",
    "\n",
    "        if num_to_discard is None:\n",
    "            raise ValueError(\"num_to_discard must be specified if not using heuristic.\")\n",
    "\n",
    "        sorted_indices = sorted(\n",
    "            range(len(group_alignment_scores)),\n",
    "            key=lambda i: group_alignment_scores[i],\n",
    "        )\n",
    "        return sorted_indices[num_to_discard:]\n",
    "\n",
    "    def debias(self, use_heuristic=True, num_to_discard=None):\n",
    "        group_losses = self.get_group_losses(\n",
    "            model=self.model,\n",
    "            val_dl=self.dataloaders[\"val\"],\n",
    "            group_indices=self.group_indices,\n",
    "        )\n",
    "\n",
    "        group_alignment_scores = self.compute_group_alignment_scores(\n",
    "            self.trak_scores, self.group_indices, group_losses\n",
    "        )\n",
    "        \n",
    "        debiased_train_inds = self.get_debiased_train_indices(\n",
    "            group_alignment_scores,\n",
    "            use_heuristic=use_heuristic,\n",
    "            num_to_discard=num_to_discard,\n",
    "        )\n",
    "\n",
    "        return debiased_train_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:35:37.386054Z",
     "iopub.status.busy": "2025-01-08T00:35:37.385812Z",
     "iopub.status.idle": "2025-01-08T00:35:37.402352Z",
     "shell.execute_reply": "2025-01-08T00:35:37.401571Z",
     "shell.execute_reply.started": "2025-01-08T00:35:37.386025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:36:13.563701Z",
     "iopub.status.busy": "2025-01-08T00:36:13.563289Z",
     "iopub.status.idle": "2025-01-08T00:37:06.572688Z",
     "shell.execute_reply": "2025-01-08T00:37:06.571716Z",
     "shell.execute_reply.started": "2025-01-08T00:36:13.563663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 187MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 512/512 [00:39<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 63/63 [00:04<00:00, 13.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 83.65%\n",
      "Training and evaluation completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to CelebA images and metadata\n",
    "celeba_images_path = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\n",
    "partition_file = \"/kaggle/input/celeba-dataset/list_eval_partition.csv\"\n",
    "attributes_file = \"/kaggle/input/celeba-dataset/list_attr_celeba.csv\"\n",
    "\n",
    "# Function to get DataLoader for CelebA\n",
    "def get_dataloader(\n",
    "        batch_size=128, num_workers=4, split=\"train\", shuffle=False, augment=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Get DataLoader for the CelebA dataset with only 10% of the total dataset.\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    if augment:\n",
    "        transforms_pipeline = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.CenterCrop(178),\n",
    "                transforms.Resize(128),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transforms_pipeline = transforms.Compose(\n",
    "            [\n",
    "                transforms.CenterCrop(178),\n",
    "                transforms.Resize(128),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Load partition and attributes\n",
    "    partitions = pd.read_csv(partition_file)\n",
    "    attributes = pd.read_csv(attributes_file)\n",
    "\n",
    "    # Ensure attributes are binary\n",
    "    attributes.iloc[:, 1:] = attributes.iloc[:, 1:].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "    total_indices = len(attributes)\n",
    "    reduced_indices = np.random.choice(attributes.index, size=total_indices // 10, replace=False)\n",
    "\n",
    "    # Define young (attribute \"Young\") and old classes\n",
    "    young_indices = attributes[attributes[\"Young\"] == 1].index.intersection(reduced_indices)\n",
    "    old_indices = attributes[attributes[\"Young\"] == 0].index.intersection(reduced_indices)\n",
    "\n",
    "    # Create the subset with a 4:1 ratio (young:old)\n",
    "    num_old = len(old_indices)\n",
    "    num_young = min(len(young_indices), num_old * 4)\n",
    "    selected_young_indices = np.random.choice(young_indices, num_young, replace=False)\n",
    "    selected_indices = np.concatenate([selected_young_indices, old_indices])\n",
    "\n",
    "    # Split the subset based on train/val partitions\n",
    "    dataset_split = \"train\" if split == \"train\" else \"valid\"\n",
    "    if dataset_split == \"train\":\n",
    "        selected_indices = partitions[\n",
    "            (partitions[\"partition\"] == 0) & partitions.index.isin(selected_indices)\n",
    "        ].index\n",
    "    else:\n",
    "        selected_indices = partitions[\n",
    "            (partitions[\"partition\"] == 1) & partitions.index.isin(selected_indices)\n",
    "        ].index\n",
    "\n",
    "    # Custom Dataset class for CelebA\n",
    "    class CelebADataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, indices, img_dir, attributes, transform=None):\n",
    "            self.indices = indices\n",
    "            self.img_dir = img_dir\n",
    "            self.attributes = attributes\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_index = self.indices[idx]\n",
    "            img_name = self.attributes.iloc[img_index, 0]\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            # Binary class label: young (1) or old (0)\n",
    "            label = torch.tensor(self.attributes.iloc[img_index][\"Young\"], dtype=torch.long)\n",
    "            return image, label\n",
    "\n",
    "    # Create Dataset and DataLoader\n",
    "    dataset = CelebADataset(\n",
    "        indices=selected_indices,\n",
    "        img_dir=celeba_images_path,\n",
    "        attributes=attributes,\n",
    "        transform=transforms_pipeline\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset, shuffle=shuffle, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return loader, dataset\n",
    "\n",
    "# Load pre-trained model\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model_before_mitigating = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model_before_mitigating.fc = nn.Linear(model_before_mitigating.fc.in_features, 2)  # Binary classification (young or old)\n",
    "model_before_mitigating = model_before_mitigating.cuda()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_before_mitigating.parameters(), lr=0.001)\n",
    "\n",
    "# Get DataLoaders\n",
    "train_loader, train_dataset = get_dataloader(batch_size=32, split=\"train\", shuffle=True)\n",
    "val_loader, val_dataset = get_dataloader(batch_size=32, split=\"val\", shuffle=False, augment=False)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    model_before_mitigating.train()\n",
    "    epoch_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_before_mitigating(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model_before_mitigating.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model_before_mitigating(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Final Output\n",
    "print(\"Training and evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:37:33.559900Z",
     "iopub.status.busy": "2025-01-08T00:37:33.559586Z",
     "iopub.status.idle": "2025-01-08T00:37:34.062574Z",
     "shell.execute_reply": "2025-01-08T00:37:34.061646Z",
     "shell.execute_reply.started": "2025-01-08T00:37:33.559876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_worst_group_accuracy(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "    group_labels = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating WGA\")):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` since labels are not one-hot encoded\n",
    "\n",
    "            # Predict using the model\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Assign predictions and labels to the respective group\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])  # Add single prediction\n",
    "                group_labels[group].append(labels.cpu().numpy()[i])  # Add single label\n",
    "\n",
    "    group_accuracies = {}\n",
    "    for group in group_preds.keys():\n",
    "        if len(group_preds[group]) == 0 or len(group_labels[group]) == 0:\n",
    "            # Skip groups with no samples\n",
    "            group_accuracies[group] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Convert lists to arrays\n",
    "        preds = np.array(group_preds[group])\n",
    "        truths = np.array(group_labels[group])\n",
    "        group_accuracies[group] = accuracy_score(truths, preds)\n",
    "\n",
    "    # Print all group accuracies\n",
    "    for group, acc in group_accuracies.items():\n",
    "        print(f\"Group {group} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Find the worst group accuracy\n",
    "    worst_group_accuracy = min(group_accuracies.values())\n",
    "    return worst_group_accuracy, group_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T01:03:50.196904Z",
     "iopub.status.busy": "2025-01-08T01:03:50.196609Z",
     "iopub.status.idle": "2025-01-08T01:03:55.338311Z",
     "shell.execute_reply": "2025-01-08T01:03:55.337413Z",
     "shell.execute_reply.started": "2025-01-08T01:03:50.196882Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Distribution in Validation Set:\n",
      "subgroup\n",
      "young-female    931\n",
      "young-male      551\n",
      "old-male        315\n",
      "old-female      191\n",
      "Name: count, dtype: int64\n",
      "Sample Group Indices: [3, 3, 2, 0, 1, 2, 1, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Load attributes\n",
    "attributes = pd.read_csv(attributes_file)\n",
    "attributes.iloc[:, 1:] = attributes.iloc[:, 1:].map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# Define subgroups based on Young and Male attributes\n",
    "def define_subgroups(row):\n",
    "    if row[\"Young\"] == 1 and row[\"Male\"] == 1:\n",
    "        return \"young-male\"\n",
    "    elif row[\"Young\"] == 1 and row[\"Male\"] == 0:\n",
    "        return \"young-female\"\n",
    "    elif row[\"Young\"] == 0 and row[\"Male\"] == 1:\n",
    "        return \"old-male\"\n",
    "    elif row[\"Young\"] == 0 and row[\"Male\"] == 0:\n",
    "        return \"old-female\"\n",
    "\n",
    "# Assign subgroup labels\n",
    "attributes[\"subgroup\"] = attributes.apply(define_subgroups, axis=1)\n",
    "\n",
    "# Map subgroup names to numerical indices\n",
    "subgroup_mapping = {name: i for i, name in enumerate(sorted(attributes[\"subgroup\"].unique()))}\n",
    "attributes[\"group_index\"] = attributes[\"subgroup\"].map(subgroup_mapping)\n",
    "\n",
    "# Align group indices with the reduced validation dataset\n",
    "val_indices = val_loader.dataset.indices  # Indices of the validation subset\n",
    "group_labels = attributes.loc[val_indices, \"group_index\"].values  # Get subgroup indices for validation set\n",
    "group_inds = list(group_labels)  # Convert to a list for compatibility if needed\n",
    "\n",
    "# Print subgroup distribution and sample group indices for validation\n",
    "print(\"Subgroup Distribution in Validation Set:\")\n",
    "print(attributes.loc[val_indices, \"subgroup\"].value_counts())\n",
    "print(f\"Sample Group Indices: {group_inds[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Fairness Metrics for Young and Old Groups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T01:04:03.955080Z",
     "iopub.status.busy": "2025-01-08T01:04:03.954771Z",
     "iopub.status.idle": "2025-01-08T01:04:03.971609Z",
     "shell.execute_reply": "2025-01-08T01:04:03.970770Z",
     "shell.execute_reply.started": "2025-01-08T01:04:03.955051Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate Group Accuracies\n",
    "def evaluate_group_accuracies(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_preds = {g: [] for g in set(group_labels)}\n",
    "    group_truths = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating Group Accuracies\")):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` here\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Assign predictions and truths to respective groups\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[j])\n",
    "                group_truths[group].append(labels.cpu().numpy()[j])\n",
    "\n",
    "    group_accuracies = {}\n",
    "    for group in group_preds:\n",
    "        if len(group_preds[group]) == 0:\n",
    "            group_accuracies[group] = 0.0\n",
    "        else:\n",
    "            preds = np.array(group_preds[group])\n",
    "            truths = np.array(group_truths[group])\n",
    "            group_accuracies[group] = accuracy_score(truths, preds)\n",
    "\n",
    "    # Print group accuracies\n",
    "    for group, acc in group_accuracies.items():\n",
    "        print(f\"Group {group} Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    return group_accuracies\n",
    "\n",
    "# Function to evaluate Demographic Parity (DP)\n",
    "def evaluate_demographic_parity(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_pprs = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(tqdm(val_loader, desc=\"Evaluating Demographic Parity\")):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(preds)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                group_pprs[group].append(preds[j])\n",
    "\n",
    "    ppr_disparities = {}\n",
    "    for group in group_pprs:\n",
    "        group_positive_rate = np.mean(group_pprs[group])\n",
    "        ppr_disparities[group] = group_positive_rate\n",
    "\n",
    "    # Print group PPRs\n",
    "    for group, ppr in ppr_disparities.items():\n",
    "        print(f\"Group {group} PPR: {ppr:.4f}\")\n",
    "    \n",
    "    return ppr_disparities\n",
    "\n",
    "# Function to evaluate Equal Opportunity (EO)\n",
    "def evaluate_equal_opportunity(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_tprs = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating Equal Opportunity\")):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` here\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                tp = (preds[j] == 1 and labels[j].cpu().numpy() == 1)\n",
    "                actual_positive = labels[j].cpu().numpy() == 1\n",
    "                group_tprs[group].append(tp / (actual_positive + 1e-8))  # Avoid division by zero\n",
    "\n",
    "    tpr_disparities = {}\n",
    "    for group in group_tprs:\n",
    "        tpr_disparities[group] = np.mean(group_tprs[group])\n",
    "\n",
    "    # Print group TPRs\n",
    "    for group, tpr in tpr_disparities.items():\n",
    "        print(f\"Group {group} TPR: {tpr:.4f}\")\n",
    "    \n",
    "    return tpr_disparities\n",
    "\n",
    "# Function to evaluate Equalized Odds (EOd)\n",
    "def evaluate_equalized_odds(model, val_loader, group_labels, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_tprs = {g: [] for g in set(group_labels)}\n",
    "    group_fprs = {g: [] for g in set(group_labels)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating Equalized Odds\")):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)  # Remove `.argmax(dim=1)` here\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = i * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_labels[batch_start:batch_end]\n",
    "\n",
    "            for j, group in enumerate(batch_groups):\n",
    "                tp = (preds[j] == 1 and labels[j].cpu().numpy() == 1)\n",
    "                fp = (preds[j] == 1 and labels[j].cpu().numpy() == 0)\n",
    "                actual_positive = labels[j].cpu().numpy() == 1\n",
    "                actual_negative = labels[j].cpu().numpy() == 0\n",
    "\n",
    "                group_tprs[group].append(tp / (actual_positive + 1e-8))  # Avoid division by zero\n",
    "                group_fprs[group].append(fp / (actual_negative + 1e-8))  # Avoid division by zero\n",
    "\n",
    "    tpr_disparities = {}\n",
    "    fpr_disparities = {}\n",
    "    for group in group_tprs:\n",
    "        tpr_disparities[group] = np.mean(group_tprs[group])\n",
    "        fpr_disparities[group] = np.mean(group_fprs[group])\n",
    "\n",
    "    # Print group TPRs and FPRs\n",
    "    for group in group_tprs:\n",
    "        print(f\"Group {group} TPR: {tpr_disparities[group]:.4f}, FPR: {fpr_disparities[group]:.4f}\")\n",
    "    \n",
    "    return tpr_disparities, fpr_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T01:04:04.516090Z",
     "iopub.status.busy": "2025-01-08T01:04:04.515802Z",
     "iopub.status.idle": "2025-01-08T01:04:12.890898Z",
     "shell.execute_reply": "2025-01-08T01:04:12.889840Z",
     "shell.execute_reply.started": "2025-01-08T01:04:04.516068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 63/63 [00:02<00:00, 27.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 Accuracy: 0.4450\n",
      "Group 1 Accuracy: 0.8190\n",
      "Group 2 Accuracy: 0.9452\n",
      "Group 3 Accuracy: 0.7985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Demographic Parity: 100%|██████████| 63/63 [00:01<00:00, 31.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 PPR: 0.5550\n",
      "Group 1 PPR: 0.1810\n",
      "Group 2 PPR: 0.9452\n",
      "Group 3 PPR: 0.7985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Equal Opportunity: 100%|██████████| 63/63 [00:02<00:00, 31.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 TPR: 0.0000\n",
      "Group 1 TPR: 0.0000\n",
      "Group 2 TPR: 0.9452\n",
      "Group 3 TPR: 0.7985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Equalized Odds: 100%|██████████| 63/63 [00:02<00:00, 31.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 TPR: 0.0000, FPR: 0.5550\n",
      "Group 1 TPR: 0.0000, FPR: 0.1810\n",
      "Group 2 TPR: 0.9452, FPR: 0.0000\n",
      "Group 3 TPR: 0.7985, FPR: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wga, group_accuracies = evaluate_worst_group_accuracy(model_before_mitigating, val_loader, group_labels)\n",
    "dp_rates = evaluate_demographic_parity(model_before_mitigating, val_loader, group_labels)\n",
    "eo_tprs = evaluate_equal_opportunity(model_before_mitigating, val_loader, group_labels)\n",
    "tpr_disparities, fpr_disparities = evaluate_equalized_odds(model_before_mitigating, val_loader, group_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing with D3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T01:04:16.962587Z",
     "iopub.status.busy": "2025-01-08T01:04:16.962189Z",
     "iopub.status.idle": "2025-01-08T01:06:06.055203Z",
     "shell.execute_reply": "2025-01-08T01:06:06.054223Z",
     "shell.execute_reply.started": "2025-01-08T01:04:16.962559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOYO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing features for all model IDs..: 100%|██████████| 1/1 [00:00<00:00, 4405.78it/s]\n",
      "Finalizing scores for all model IDs..: 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "print('YOYO')\n",
    "ckpts = [model_before_mitigating.state_dict()]\n",
    "dda = DDA(model_before_mitigating, ckpts, train_loader, val_loader, group_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T01:06:20.995008Z",
     "iopub.status.busy": "2025-01-08T01:06:20.994689Z",
     "iopub.status.idle": "2025-01-08T01:06:23.636328Z",
     "shell.execute_reply": "2025-01-08T01:06:23.635376Z",
     "shell.execute_reply.started": "2025-01-08T01:06:20.994986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16275"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debiased_inds = dda.debias(use_heuristic=False, num_to_discard=100)\n",
    "debiased_inds = dda.debias(use_heuristic=True)\n",
    "len(debiased_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:16:46.369086Z",
     "iopub.status.busy": "2025-01-08T00:16:46.368521Z",
     "iopub.status.idle": "2025-01-08T00:16:46.400823Z",
     "shell.execute_reply": "2025-01-08T00:16:46.399947Z",
     "shell.execute_reply.started": "2025-01-08T00:16:46.369023Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "deep_copy_model = copy.deepcopy(model_before_mitigating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:54:21.458707Z",
     "iopub.status.busy": "2025-01-07T17:54:21.458404Z",
     "iopub.status.idle": "2025-01-07T17:54:21.467263Z",
     "shell.execute_reply": "2025-01-07T17:54:21.466222Z",
     "shell.execute_reply.started": "2025-01-07T17:54:21.458682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_tpr(labels, preds):\n",
    "    tp = np.sum((preds == 1) & (labels == 1))\n",
    "    fn = np.sum((preds == 0) & (labels == 1))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "def evaluate_equal_opportunity(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "    group_labels = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])\n",
    "                group_labels[group].append(labels.cpu().numpy()[i])\n",
    "\n",
    "    group_tprs = {}\n",
    "    for group in group_preds.keys():\n",
    "        preds = np.array(group_preds[group])\n",
    "        labels = np.array(group_labels[group])\n",
    "        group_tprs[group] = calculate_tpr(labels, preds)\n",
    "\n",
    "    min_tpr = min(group_tprs.values())\n",
    "    max_tpr = max(group_tprs.values())\n",
    "    tpr_disparity = max_tpr - min_tpr\n",
    "\n",
    "    return group_tprs, tpr_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:54:22.857763Z",
     "iopub.status.busy": "2025-01-07T17:54:22.857441Z",
     "iopub.status.idle": "2025-01-07T17:54:33.086290Z",
     "shell.execute_reply": "2025-01-07T17:54:33.085093Z",
     "shell.execute_reply.started": "2025-01-07T17:54:22.857736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group TPRs: {0: 0.9152671755725191, 1: 0.9867026802410139, 2: 0.0, 3: 0.0}\n",
      "TPR Disparity: 0.9867\n"
     ]
    }
   ],
   "source": [
    "group_tprs, tpr_disparity = evaluate_equal_opportunity(deep_copy_model, val_loader, group_inds)\n",
    "print(f\"Group TPRs: {group_tprs}\")\n",
    "print(f\"TPR Disparity: {tpr_disparity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:54:54.856637Z",
     "iopub.status.busy": "2025-01-07T17:54:54.856293Z",
     "iopub.status.idle": "2025-01-07T17:54:54.864961Z",
     "shell.execute_reply": "2025-01-07T17:54:54.863960Z",
     "shell.execute_reply.started": "2025-01-07T17:54:54.856606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_fpr(labels, preds):\n",
    "    fp = np.sum((preds == 1) & (labels == 0))\n",
    "    tn = np.sum((preds == 0) & (labels == 0))\n",
    "    return fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "def evaluate_equalized_odds(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "    group_labels = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])\n",
    "                group_labels[group].append(labels.cpu().numpy()[i])\n",
    "\n",
    "    group_tprs, group_fprs = {}, {}\n",
    "    for group in group_preds.keys():\n",
    "        preds = np.array(group_preds[group])\n",
    "        labels = np.array(group_labels[group])\n",
    "        group_tprs[group] = calculate_tpr(labels, preds)\n",
    "        group_fprs[group] = calculate_fpr(labels, preds)\n",
    "\n",
    "    tpr_disparity = max(group_tprs.values()) - min(group_tprs.values())\n",
    "    fpr_disparity = max(group_fprs.values()) - min(group_fprs.values())\n",
    "\n",
    "    return group_tprs, group_fprs, tpr_disparity, fpr_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:54:57.857798Z",
     "iopub.status.busy": "2025-01-07T17:54:57.857512Z",
     "iopub.status.idle": "2025-01-07T17:55:06.649775Z",
     "shell.execute_reply": "2025-01-07T17:55:06.648806Z",
     "shell.execute_reply.started": "2025-01-07T17:54:57.857775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group TPRs: {0: 0.9152671755725191, 1: 0.9867026802410139, 2: 0.0, 3: 0.0}\n",
      "Group FPRs: {0: 0.0, 1: 0.0, 2: 0.2816989381636477, 3: 0.6818181818181818}\n",
      "TPR Disparity: 0.9867\n",
      "FPR Disparity: 0.6818\n"
     ]
    }
   ],
   "source": [
    "group_tprs, group_fprs, tpr_disparity, fpr_disparity = evaluate_equalized_odds(deep_copy_model, val_loader, group_inds)\n",
    "print(f\"Group TPRs: {group_tprs}\")\n",
    "print(f\"Group FPRs: {group_fprs}\")\n",
    "print(f\"TPR Disparity: {tpr_disparity:.4f}\")\n",
    "print(f\"FPR Disparity: {fpr_disparity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:55:06.651435Z",
     "iopub.status.busy": "2025-01-07T17:55:06.651100Z",
     "iopub.status.idle": "2025-01-07T17:55:06.658354Z",
     "shell.execute_reply": "2025-01-07T17:55:06.657597Z",
     "shell.execute_reply.started": "2025-01-07T17:55:06.651400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_ppr(preds):\n",
    "    return np.mean(preds)\n",
    "\n",
    "\n",
    "def evaluate_demographic_parity(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate Demographic Parity.\n",
    "    Ensures PPR is correctly normalized as probabilities.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    group_preds = {i: [] for i in set(group_inds)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            # Get the group indices for the current batch\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            # Assign predictions to the corresponding group\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                group_preds[group].append(preds[i])\n",
    "\n",
    "    group_pprs = {}\n",
    "    for group in group_preds.keys():\n",
    "        # Flatten the predictions list for each group and normalize\n",
    "        preds = np.array(group_preds[group]).flatten()\n",
    "        positive_preds = (preds == 1).sum()  # Count positive predictions\n",
    "        total_preds = len(preds)  # Total number of predictions\n",
    "        group_pprs[group] = positive_preds / total_preds if total_preds > 0 else 0.0\n",
    "\n",
    "    # Calculate the disparity in PPRs across groups\n",
    "    min_ppr = min(group_pprs.values())\n",
    "    max_ppr = max(group_pprs.values())\n",
    "    ppr_disparity = max_ppr - min_ppr\n",
    "\n",
    "    return group_pprs, ppr_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:55:14.315853Z",
     "iopub.status.busy": "2025-01-07T17:55:14.315511Z",
     "iopub.status.idle": "2025-01-07T17:55:23.086808Z",
     "shell.execute_reply": "2025-01-07T17:55:23.085765Z",
     "shell.execute_reply.started": "2025-01-07T17:55:14.315822Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group PPRs: {0: 0.9152671755725191, 1: 0.9867026802410139, 2: 0.2816989381636477, 3: 0.6818181818181818}\n",
      "PPR Disparity: 0.7050\n"
     ]
    }
   ],
   "source": [
    "group_pprs, ppr_disparity = evaluate_demographic_parity(deep_copy_model, val_loader, group_inds)\n",
    "print(f\"Group PPRs: {group_pprs}\")\n",
    "print(f\"PPR Disparity: {ppr_disparity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:55:23.088539Z",
     "iopub.status.busy": "2025-01-07T17:55:23.088270Z",
     "iopub.status.idle": "2025-01-07T17:55:32.165376Z",
     "shell.execute_reply": "2025-01-07T17:55:32.164066Z",
     "shell.execute_reply.started": "2025-01-07T17:55:23.088516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating FNR and FPR: 100%|██████████| 312/312 [00:09<00:00, 34.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group FNR and FPR:\n",
      "Group 0: FNR = 0.0847, FPR = 0.0000\n",
      "Group 1: FNR = 0.0133, FPR = 0.0000\n",
      "Group 2: FNR = 0.0000, FPR = 0.2817\n",
      "Group 3: FNR = 0.0000, FPR = 0.6818\n",
      "\n",
      "FNR Disparity (Max - Min): 0.0847\n",
      "FPR Disparity (Max - Min): 0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_fnr_fpr(model, val_loader, group_inds, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Calculate False Negative Rate (FNR) and False Positive Rate (FPR) for each group.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    group_metrics = {g: {\"FN\": 0, \"FP\": 0, \"TP\": 0, \"TN\": 0} for g in set(group_inds)}  # Metrics for each group\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(val_loader, desc=\"Calculating FNR and FPR\")):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            # Get the groups for the current batch\n",
    "            batch_start = batch_idx * val_loader.batch_size\n",
    "            batch_end = batch_start + len(labels)\n",
    "            batch_groups = group_inds[batch_start:batch_end]\n",
    "\n",
    "            for i, group in enumerate(batch_groups):\n",
    "                if group not in group_metrics:\n",
    "                    continue  # Skip if group is not defined\n",
    "                \n",
    "                # Update confusion matrix components\n",
    "                if labels[i] == 1 and preds[i] == 0:  # False Negative\n",
    "                    group_metrics[group][\"FN\"] += 1\n",
    "                elif labels[i] == 0 and preds[i] == 1:  # False Positive\n",
    "                    group_metrics[group][\"FP\"] += 1\n",
    "                elif labels[i] == 1 and preds[i] == 1:  # True Positive\n",
    "                    group_metrics[group][\"TP\"] += 1\n",
    "                elif labels[i] == 0 and preds[i] == 0:  # True Negative\n",
    "                    group_metrics[group][\"TN\"] += 1\n",
    "\n",
    "    # Calculate FNR and FPR for each group\n",
    "    group_fnr_fpr = {}\n",
    "    for group, metrics in group_metrics.items():\n",
    "        fn = metrics[\"FN\"]\n",
    "        fp = metrics[\"FP\"]\n",
    "        tp = metrics[\"TP\"]\n",
    "        tn = metrics[\"TN\"]\n",
    "\n",
    "        actual_positives = tp + fn\n",
    "        actual_negatives = tn + fp\n",
    "\n",
    "        fnr = fn / (actual_positives + 1e-8) if actual_positives > 0 else 0.0\n",
    "        fpr = fp / (actual_negatives + 1e-8) if actual_negatives > 0 else 0.0\n",
    "\n",
    "        group_fnr_fpr[group] = {\"FNR\": fnr, \"FPR\": fpr}\n",
    "\n",
    "    # Print FNR and FPR for each group\n",
    "    print(\"\\nGroup FNR and FPR:\")\n",
    "    for group, metrics in group_fnr_fpr.items():\n",
    "        print(f\"Group {group}: FNR = {metrics['FNR']:.4f}, FPR = {metrics['FPR']:.4f}\")\n",
    "\n",
    "    # Calculate and print disparities\n",
    "    fnr_values = [metrics[\"FNR\"] for metrics in group_fnr_fpr.values()]\n",
    "    fpr_values = [metrics[\"FPR\"] for metrics in group_fnr_fpr.values()]\n",
    "    fnr_disparity = max(fnr_values) - min(fnr_values)\n",
    "    fpr_disparity = max(fpr_values) - min(fpr_values)\n",
    "\n",
    "    print(f\"\\nFNR Disparity (Max - Min): {fnr_disparity:.4f}\")\n",
    "    print(f\"FPR Disparity (Max - Min): {fpr_disparity:.4f}\")\n",
    "\n",
    "    return group_fnr_fpr, fnr_disparity, fpr_disparity\n",
    "\n",
    "\n",
    "# Example usage\n",
    "group_fnr_fpr, fnr_disparity, fpr_disparity = calculate_fnr_fpr(model_before_mitigating, val_loader, group_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T00:17:12.505727Z",
     "iopub.status.busy": "2025-01-08T00:17:12.505393Z",
     "iopub.status.idle": "2025-01-08T00:17:12.509819Z",
     "shell.execute_reply": "2025-01-08T00:17:12.508709Z",
     "shell.execute_reply.started": "2025-01-08T00:17:12.505696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "harmful_indices = debiased_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T19:21:35.483585Z",
     "iopub.status.busy": "2025-01-07T19:21:35.483278Z",
     "iopub.status.idle": "2025-01-07T19:38:07.986398Z",
     "shell.execute_reply": "2025-01-07T19:38:07.985362Z",
     "shell.execute_reply.started": "2025-01-07T19:21:35.483560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 312/312 [00:10<00:00, 30.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 Accuracy: 0.9080\n",
      "Group 1 Accuracy: 0.9838\n",
      "Group 2 Accuracy: 0.7408\n",
      "Group 3 Accuracy: 0.3734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating WGA: 100%|██████████| 312/312 [00:09<00:00, 31.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 Accuracy: 0.9466\n",
      "Group 1 Accuracy: 0.9877\n",
      "Group 2 Accuracy: 0.6921\n",
      "Group 3 Accuracy: 0.3690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_influence(model, dataloader, harmful_indices, factor, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    harmful_dataset = torch.utils.data.Subset(dataloader.dataset, harmful_indices)\n",
    "    harmful_loader = torch.utils.data.DataLoader(harmful_dataset, batch_size=1)\n",
    "\n",
    "    for inputs, labels in harmful_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(model.parameters(), grads):\n",
    "                param -= grad * factor\n",
    "\n",
    "    return model\n",
    "\n",
    "results ={'factor':[], 'model':[], 'min':[], 'max':[], 'gap':[]}\n",
    "factors = np.linspace(0.00001, 0.0001, 2)\n",
    "\n",
    "for factor in factors:\n",
    "    newdeepmodel = copy.deepcopy(deep_copy_model)\n",
    "    m = remove_influence(newdeepmodel, train_loader, harmful_indices, factor, device=\"cuda\")\n",
    "    wga, group_accs = evaluate_worst_group_accuracy(m, val_loader, group_inds, device=\"cuda\")\n",
    "    current_gap = (max(group_accs.values()) - wga)\n",
    "    results['model'].append(m)\n",
    "    results['min'].append(wga)\n",
    "    results['max'].append(max(group_accs.values()))\n",
    "    results['gap'].append(current_gap)\n",
    "    results['factor'].append(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T19:40:09.687904Z",
     "iopub.status.busy": "2025-01-07T19:40:09.687564Z",
     "iopub.status.idle": "2025-01-07T19:40:09.702851Z",
     "shell.execute_reply": "2025-01-07T19:40:09.702144Z",
     "shell.execute_reply.started": "2025-01-07T19:40:09.687877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factor</th>\n",
       "      <th>model</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>ResNet(\\n  (conv1): Conv2d(3, 64, kernel_size=...</td>\n",
       "      <td>0.373377</td>\n",
       "      <td>0.983794</td>\n",
       "      <td>0.610417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>ResNet(\\n  (conv1): Conv2d(3, 64, kernel_size=...</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.987742</td>\n",
       "      <td>0.618694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    factor                                              model       min  \\\n",
       "0  0.00001  ResNet(\\n  (conv1): Conv2d(3, 64, kernel_size=...  0.373377   \n",
       "1  0.00010  ResNet(\\n  (conv1): Conv2d(3, 64, kernel_size=...  0.369048   \n",
       "\n",
       "        max       gap  \n",
       "0  0.983794  0.610417  \n",
       "1  0.987742  0.618694  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results).sort_values('factor')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to investigate what are the best approaches to machine unlearning and how can we formulate that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the other approaches to machine unlearning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "def fair_pruning(model, dataloader, harmful_indices, threshold=0.01, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    harmful_dataset = torch.utils.data.Subset(dataloader.dataset, harmful_indices)\n",
    "    harmful_loader = torch.utils.data.DataLoader(harmful_dataset, batch_size=1)\n",
    "\n",
    "    parameter_gradients = []\n",
    "    for inputs, labels in tqdm(harmful_loader, desc=\"Calculating Gradients\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = pruned_model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        grads = torch.autograd.grad(loss, pruned_model.parameters(), retain_graph=True)\n",
    "        parameter_gradients.append([grad.clone() for grad in grads])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for param, grads in zip(pruned_model.parameters(), zip(*parameter_gradients)):\n",
    "            mean_grad = torch.mean(torch.stack(grads), dim=0)\n",
    "            param[torch.abs(mean_grad) < threshold] = 0.0\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "pruned_model = fair_pruning(model_before_mitigating, train_loader, harmful_indices, threshold=0.01)\n",
    "\n",
    "wga, group_accs = evaluate_worst_group_accuracy(pruned_model, val_loader, group_inds, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentially Private Influence Functions for Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def dp_influence_unlearning(model, dataloader, harmful_indices, epsilon=1.0, delta=1e-5, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    updated_model = copy.deepcopy(model)\n",
    "    harmful_dataset = torch.utils.data.Subset(dataloader.dataset, harmful_indices)\n",
    "    harmful_loader = torch.utils.data.DataLoader(harmful_dataset, batch_size=1)\n",
    "\n",
    "    sensitivity = 1.0 / len(harmful_loader)\n",
    "    noise_scale = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n",
    "\n",
    "    for inputs, labels in tqdm(harmful_loader, desc=\"Applying DP Influence Unlearning\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = updated_model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        grads = torch.autograd.grad(loss, updated_model.parameters(), retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(updated_model.parameters(), grads):\n",
    "                noise = torch.normal(mean=0, std=noise_scale, size=grad.shape, device=device)\n",
    "                param -= (grad + noise)\n",
    "\n",
    "    return updated_model\n",
    "\n",
    "dp_model = dp_influence_unlearning(model_before_mitigating, train_loader, harmful_indices, epsilon=1.0, delta=1e-5)\n",
    "wga, group_accs = evaluate_worst_group_accuracy(dp_model, val_loader, group_inds, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
